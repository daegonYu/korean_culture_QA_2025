from __init__ import *

class PromptingExperiment:
    def __init__(self, model_name="beomi/Kanana-8B", load_model=True, use_lora=False, use_wandb=False, \
                system_prompt="", user_prompt="", answer_tag="ì •ë‹µ:", max_lora_rank=64):
        """
        Phase 1: í”„ë¡¬í”„íŒ… ì‹¤í—˜
        5ê°€ì§€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ Kanana 8B ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        """

        self.model_name = model_name
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt
        self.answer_tag = answer_tag
        self.use_lora = use_lora
        self.use_wandb = use_wandb
        self.max_lora_rank = max_lora_rank
        # wandb ì´ˆê¸°í™”
        if self.use_wandb:
            load_dotenv()
            wandb_api_key = os.getenv("WANDB_API_KEY")

            wandb.login(key=wandb_api_key)

            wandb.init(
                project="moducorpus_korea_culture_answer_log",
                name=f"{model_name.replace('/', '_')}",
                config={
                    "model_name": model_name,
                    "load_model": load_model,
                    "use_lora": self.use_lora,
                    "system_prompt": system_prompt,
                    "user_prompt_template": user_prompt,
                    "answer_tag": answer_tag
                }
            )
            self.wb_table = wandb.Table(columns=["prompt_system", "prompt_user", "answer"])

        self.llm = None
        self.lora_req = None
        max_model_len=800
        if load_model and not self.use_lora:
            print(f"Loading {self.model_name}...")
            self.llm = LLM(
                model=self.model_name,
                dtype="bfloat16",     # ë˜ëŠ” "float16", "auto"
                trust_remote_code=True,
                max_model_len=max_model_len,   # ìµœëŒ€ ì…ë ¥ ê¸¸ì´
                gpu_memory_utilization=0.9,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            )
        elif load_model and self.use_lora:
            config = PeftConfig.from_pretrained(self.model_name)
            base_model_name = config.base_model_name_or_path
            print(f"Loading {self.model_name} with LoRA Layers...")
            self.llm = LLM(
                model=base_model_name,
                dtype="bfloat16",     # ë˜ëŠ” "float16", "auto"
                trust_remote_code=True,
                max_model_len=max_model_len,   # ìµœëŒ€ ì…ë ¥ ê¸¸ì´
                gpu_memory_utilization=0.9,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                enable_lora=self.use_lora,
                max_lora_rank=self.max_lora_rank
            )
            self.lora_req = LoRARequest("adapter", 1, self.model_name)

        # Sampling íŒŒë¼ë¯¸í„° ì„¤ì • (ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ì¤€ë¹„)
        self.sampling_params = SamplingParams(
            max_tokens=max_model_len,
            temperature=0.6,
            top_p=0.95
        )
                
    def load_data(self, data_path="data"):
        """ë°ì´í„° ë¡œë“œ"""
        data_dir = Path(data_path)
        
        with open(data_dir / "train.json", "r", encoding="utf-8") as f:
            train_data = json.load(f)
        with open(data_dir / "dev.json", "r", encoding="utf-8") as f:
            dev_data = json.load(f)
        with open(data_dir / "preprocessed/test.json", "r", encoding="utf-8") as f:
            test_data = json.load(f)
        
        print(f"Loaded {len(train_data)} train, {len(dev_data)} dev" + (f", {len(test_data)} test" if test_data else ""))
        return train_data, dev_data, test_data
    
    def create_prompts(self, sample):
        """5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question = sample['input']['question']
        question_type = sample['input']['question_type']
        category = sample['input']['category']
        domain = sample['input']['domain']
        topic_keyword = sample['input']['topic_keyword']
        
        prompts = {}

        system_prompt = self.system_prompt
        user_prompt = self.user_prompt
        user_prompt = user_prompt.format(
            category=category,
            domain=domain,
            topic_keyword=topic_keyword,
            question_type=question_type,
            question=question
        )
        print(f"user_prompt:{user_prompt}")


        # 0. System prompt: ì „ë¬¸ê°€ ì—­í•  ë¶€ì—¬
#         system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ë¬¸í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì ì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
# ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
# 1. **ì„ ë‹¤í˜• (Multiple Choice)**  
#    - ë³´ê¸° ì¤‘ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë²ˆí˜¸ë§Œ **ìˆ«ì**ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

# 2. **ë‹¨ë‹µí˜• (Short Answer)**  
#    - 5ì–´ì ˆ ì´ë‚´ì˜ **ëª…ì‚¬ ë˜ëŠ” êµ¬**ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.  

# 3. **ì„œìˆ í˜• (Descriptive Answer)**  
#    - 500ì ì´ë‚´ì˜ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤."""

#         detailed_system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ë¬¸í™”ì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ì‹ ë¢°ë„ ë†’ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œì— ê°€ì¥ ì í•©í•œ ì •ë‹µì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤:
# - ì¹´í…Œê³ ë¦¬(category) ë° ë„ë©”ì¸(domain): ì§ˆë¬¸ì´ ì†í•œ ì „ë°˜ì ì¸ ì§€ì‹ ë¶„ì•¼
# - ì£¼ì œ(topic_keyword): ë¬¸ì œì˜ í•µì‹¬ í‚¤ì›Œë“œ
# - ì§ˆë¬¸ ìœ í˜•(question_type): 'ì„ ë‹¤í˜•', 'ë‹¨ë‹µí˜•', ë˜ëŠ” 'ì„œìˆ í˜•' ì¤‘ í•˜ë‚˜
# - ì§ˆë¬¸ ë‚´ìš©(question): ì‚¬ìš©ìê°€ ì§ì ‘ ë¬»ëŠ” ì§ˆë¬¸

# ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
# 1. **ì„ ë‹¤í˜• (Multiple Choice)**  
#    - ë³´ê¸° ì¤‘ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë²ˆí˜¸ë§Œ **ìˆ«ì**ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

# 2. **ë‹¨ë‹µí˜• (Short Answer)**  
#    - 5ì–´ì ˆ ì´ë‚´ì˜ **ëª…ì‚¬ ë˜ëŠ” êµ¬**ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.  

# 3. **ì„œìˆ í˜• (Descriptive Answer)**  
#    - 500ì ì´ë‚´ì˜ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤."""

        # reasoning_start = "<think>" 
        # reasoning_end   = "</think>"
        # solution_start  = "<answer>"
        # solution_end    = "</answer>"

#         grpo_v1_system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì˜ ë¬¸í™”ì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ì‹ ë¢°ë„ ë†’ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œì— ê°€ì¥ ì í•©í•œ ì •ë‹µì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤:
# - ì¹´í…Œê³ ë¦¬(category) ë° ë„ë©”ì¸(domain): ì§ˆë¬¸ì´ ì†í•œ ì „ë°˜ì ì¸ ì§€ì‹ ë¶„ì•¼
# - ì£¼ì œ(topic_keyword): ë¬¸ì œì˜ í•µì‹¬ í‚¤ì›Œë“œ
# - ì§ˆë¬¸ ìœ í˜•(question_type): 'ì„ ë‹¤í˜•', 'ë‹¨ë‹µí˜•', ë˜ëŠ” 'ì„œìˆ í˜•' ì¤‘ í•˜ë‚˜
# - ì§ˆë¬¸ ë‚´ìš©(question): ì‚¬ìš©ìê°€ ì§ì ‘ ë¬»ëŠ” ì§ˆë¬¸

# ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
# 1. **ì„ ë‹¤í˜• (Multiple Choice)**  
# - ë³´ê¸° ì¤‘ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë²ˆí˜¸ë§Œ **ìˆ«ì**ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

# 2. **ë‹¨ë‹µí˜• (Short Answer)**  
# - 5ì–´ì ˆ ì´ë‚´ì˜ **ëª…ì‚¬ ë˜ëŠ” êµ¬**ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.  

# 3. **ì„œìˆ í˜• (Descriptive Answer)**  
# - 500ì ì´ë‚´ì˜ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.

# ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  ë‹µì„ ì¶”ë¡ í•œ ê³¼ì •ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤:
# {reasoning_start}
# ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¶”ë¡  ê³¼ì •ì„ í•œêµ­ì–´ë¡œ ì„œìˆ í•©ë‹ˆë‹¤.

# ìµœì¢… ì •ë‹µì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤:
# {solution_start}
# ìœ„ ì‘ì„±ëœ ë‚´ìš©ì„ í† ëŒ€ë¡œ ìµœì¢… ì •ë‹µë§Œì„ ì¶œë ¥í•©ë‹ˆë‹¤."""

        # 1. Baseline: questionë§Œ
        # prompts['baseline'] = {"system_prompt":system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\nì§ˆë¬¸: {question}\në‹µë³€:"}
        
        # 2. Simple: question_type + question
        # prompts['simple'] = {"system_prompt":system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n<{question_type}>\n<ì§ˆë¬¸>\n{question}\në‹µë³€:"}
        
        # 3. Rich: ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨
        # prompts['rich'] = {"system_prompt":system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n\në¶„ë¥˜: {category}\në„ë©”ì¸: {domain}\nì£¼ì œ: {topic_keyword}\në‹µë³€ ìœ í˜•: {question_type}\n\n<ì§ˆë¬¸>\n{question}\n\në‹µë³€:"}

        # 4. Expert: ì „ë¬¸ê°€ ì—­í•  ë¶€ì—¬
        # prompts['expert'] = {"system_prompt":system_prompt, "user_prompt":f"ë‹¹ì‹ ì€ í•œêµ­ ë¬¸í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì ì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\nì§ˆë¬¸: {question}\në‹µë³€:"}

        # 5. Format-aware: ë‹µë³€ í˜•ì‹ ëª…ì‹œ
        # if question_type == "ì„ ë‹¤í˜•":
        #     format_instruction = "ë³´ê¸° ì¤‘ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ë²ˆí˜¸ë§Œ **ìˆ«ì**ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."
        # elif question_type == "ë‹¨ë‹µí˜•":
        #     format_instruction = "5ì–´ì ˆ ì´ë‚´ì˜ **ëª…ì‚¬ ë˜ëŠ” êµ¬**ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤."
        # else:  # ì„œìˆ í˜•
        #     format_instruction = "500ì ì´ë‚´ì˜ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤."

        # prompts['format_aware'] = {"system_prompt":system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n\n<{question_type}>\n{format_instruction}\n\n<ì§ˆë¬¸>\n{question}\n\në‹µë³€:"}

        # prompts['detailed'] = {"system_prompt":detailed_system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n\ncategory: {category}\ndomain: {domain}\ntopic_keyword: {topic_keyword}\nquestion_type: {question_type}\n\n<ì§ˆë¬¸>\n{question}\n\në‹µë³€:"}

        # prompts['grpo_v1'] = {"system_prompt":grpo_v1_system_prompt, "user_prompt":f"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n\ncategory: {category}\ndomain: {domain}\ntopic_keyword: {topic_keyword}\nquestion_type: {question_type}\n\n<ì§ˆë¬¸>\n{question}\n\në‹µë³€:"}

        prompts['experiment'] = {"system_prompt":system_prompt, "user_prompt": user_prompt}
        
        return prompts
    
    def generate_answer(self, prompt):
        """ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            messages = [
                {"role": "system", "content": prompt['system_prompt']},
                {"role": "user", "content": prompt['user_prompt']}
            ]

            # ìƒì„±
            if self.use_lora:
                outputs = self.llm.chat(
                    messages=messages,
                    sampling_params=self.sampling_params,
                    lora_request=self.lora_req
                )
            else:
                outputs = self.llm.chat(
                    messages=messages,
                    sampling_params=self.sampling_params,
                )

            # ë””ì½”ë”© (ì…ë ¥ ê¸¸ì´ ì´í›„ë§Œ ì¶”ì¶œ)
            generated_text = outputs[0].outputs[0].text
            
            # ë‹µë³€ ì •ë¦¬
            answer = generated_text.strip()

            # wandb ë¡œê·¸ ì¶”ê°€
            if self.use_wandb:
                self.wb_table.add_data(
                    prompt['system_prompt'],
                    prompt['user_prompt'],
                    answer
                )

            # if '\n' in answer:
            #     answer = answer.split('\n')[0].strip()
            print(answer)
            return answer
            
        except Exception as e:
            print(f"Generation error: {e}")
            return ""
    
    def evaluate_multiple_choice(self, pred_answer, true_answer):
        """ì„ ë‹¤í˜• í‰ê°€"""
        # ìˆ«ì ì¶”ì¶œ
        # pred_nums = re.findall(r'\b[1-5]\b', pred_answer)
        pred_nums = re.findall(r'[1-5]', pred_answer)
        if pred_nums:
            pred = pred_nums[0]
        else:
            pred = pred_answer.strip()
        
        return 1 if pred == true_answer else 0
    
    def evaluate_short_answer(self, pred_answer, true_answer):
        """
        Calculate Exact Match score where true_data may contain multiple acceptable answers separated by #
        """
        correct = 0
        true_answer_list = true_answer.split('#')

        # if any(pred_answer.replace(' ','') == ans.replace(' ','') for ans in true_answer_list):
        if any(pred_answer == true_answer for true_answer in true_answer_list):
            correct = 1
                
        return correct
    
    def evaluate_long_answer(self, pred_answer, true_answer):
        """ì„œìˆ í˜• í‰ê°€ (ROUGE-1, ROUGE-2, ROUGE-L via rouge_metric)"""
        from rouge_metric import Rouge

        rouge_evaluator = Rouge(
            metrics=["rouge-n", "rouge-l"],
            max_n=2,
            limit_length=True,
            length_limit=1000,
            length_limit_type="words",
            use_tokenizer=True,
            apply_avg=True,
            apply_best=False,
            alpha=0.5,        # F1 score
            weight_factor=1.0,
        )

        # ë‹¨ì¼ ë¬¸ì¥ pairë„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
        scores = rouge_evaluator.get_scores(
            [pred_answer],
            [true_answer]
        )

        return {
            'rouge1':   scores['rouge-1']['f'],
            'rouge2':   scores['rouge-2']['f'],
            'rougeL':   scores['rouge-l']['f']
        }

    def calc_BLEU(self, pred_answer: str, true_answer: str, apply_avg=True, apply_best=False, use_mecab=True):
        from nltk.translate.bleu_score import sentence_bleu
        from konlpy.tag import Mecab

        tokenizer = Mecab()
        stacked_bleu = []

        # 1) ë‹¨ì¼ ë¬¸ìì—´ì„ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
        refs = [true_answer]  # ë¦¬ìŠ¤íŠ¸ of reference strings
        cand = pred_answer    # í•˜ë‚˜ì˜ candidate string

        # 2) í† í¬ë‚˜ì´ì§•
        if use_mecab:
            cand_tokens = tokenizer.morphs(cand)
        else:
            cand_tokens = cand.split()

        # 3) ê° ref í…ìŠ¤íŠ¸ë³„ë¡œ BLEU ê³„ì‚°
        best_bleu = 0
        sum_bleu  = 0
        for ref_text in refs:
            if use_mecab:
                ref_tokens = tokenizer.morphs(ref_text)
            else:
                ref_tokens = ref_text.split()

            score = sentence_bleu([ref_tokens], cand_tokens, weights=(1, 0, 0, 0))
            sum_bleu  += score
            best_bleu = max(best_bleu, score)

        # 4) í‰ê· 
        avg_bleu = sum_bleu / len(refs)  # ì´ì œ len(refs) == 1

        # 5) ê²°ê³¼ ìˆ˜ì§‘
        if apply_best:
            stacked_bleu.append(best_bleu)
        if apply_avg:
            stacked_bleu.append(avg_bleu)

        return sum(stacked_bleu) / len(stacked_bleu) if stacked_bleu else 0.0


    # def calc_bertscore(self, true, pred):
    #     import evaluate
    #     bert_scorer = evaluate.load('bertscore')
    #     scores = bert_scorer.compute(
    #         predictions=pred,
    #         references=true,
    #         model_type='bert-base-multilingual-cased',
    #         lang='ko',
    #         batch_size=1,
    #     )
    #     return sum(scores['f1']) / len(scores['f1'])

    # def calc_bleurt(self, true, pred):
    #     from bleurt import score
    #     scorer = score.BleurtScorer('/workspace/korean_culture_QA_2025/bleurt/BLEURT-20')
    #     # BLEURT expects flat lists of strings
    #     flat_true = [t if isinstance(t, str) else t[0] for t in true]
    #     scores = scorer.score(references=flat_true, candidates=pred, batch_size=64)
    #     return sum(scores) / len(scores)

    def run_experiment(self, data, sample_size=None, save_results=True, test_mode=False):
        """ì‹¤í—˜ ì‹¤í–‰"""
        if sample_size:
            data = data[:sample_size]
            print(f"Using {sample_size} samples for testing")
        
        results = {
            'ì„ ë‹¤í˜•': [],
            'ë‹¨ë‹µí˜•': [],
            'ì„œìˆ í˜•': []
        }
        
        for i, sample in enumerate(tqdm(data, desc="Running experiments")):
            question_type = sample['input']['question_type']
            true_answer = sample['output']['answer'] if 'output' in sample else None
            
            # 5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = self.create_prompts(sample)
            
            sample_results = {
                'id': sample['id'],
                'question_type': question_type,
                'true_answer': true_answer,
                'question': sample['input']['question']
            }
            
            # ê° í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìƒì„± ë° í‰ê°€
            for prompt_name, prompt in prompts.items():
                pred_answer = self.generate_answer(prompt=prompt)
                sample_results[f'{prompt_name}_pred'] = pred_answer
                if test_mode:
                    continue  # í‰ê°€ ìŠ¤í‚µ

                original_answer = pred_answer
                if self.answer_tag != '' and self.answer_tag in pred_answer:
                    pred_answer = pred_answer.split(self.answer_tag)[-1].strip()
                pred_answer = pred_answer.replace('*', '').replace('</answer>','')

                
                # ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê°€
                if question_type == "ì„ ë‹¤í˜•":
                    score = self.evaluate_multiple_choice(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_score'] = score
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact = self.evaluate_short_answer(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_exact'] = exact
                    # sample_results[f'{prompt_name}_partial'] = partial
                    
                else:  # ì„œìˆ í˜•
                    # 1) Rouge
                    rouge_scores = self.evaluate_long_answer(original_answer, true_answer)
                    sample_results[f'{prompt_name}_rouge1'] = rouge_scores['rouge1']
                    sample_results[f'{prompt_name}_rouge2'] = rouge_scores['rouge2']
                    sample_results[f'{prompt_name}_rougeL'] = rouge_scores['rougeL']

                    # 2) BLEU
                    bleu_score = self.calc_BLEU(original_answer, true_answer)
                    sample_results[f'{prompt_name}_bleu'] = bleu_score

                    # 3) BERTScore 
                    # bertscore = self.calc_bertscore(
                    #     [true_answer],
                    #     [pred_answer],
                    # )
                    # sample_results[f'{prompt_name}_bertscore'] = bertscore

                    # 4) BLEURT
                    # bleurt_score = self.calc_bleurt(
                    #     [true_answer],
                    #     [pred_answer],
                    # )
                    # sample_results[f'{prompt_name}_bleurt'] = bleurt_score
            
            results[question_type].append(sample_results)
            
            # ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
            # if ((i + 1) % 100 == 0) or (i + 1 == len(data)) and save_results:
            #     self.save_intermediate_results(results, i + 1)
        
        wandb.log({"prompts_and_answers": self.wb_table})
        return results
    
    def save_intermediate_results(self, results, current_idx):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        save_path = f"results/phase1_{self.model_name.split('/')[-1]}_intermediate_results_{current_idx}.json"
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def only_scoring(self, data: dict) -> dict:
        """
        data: {
        "ì„ ë‹¤í˜•": [ { "id":..., "question_type":"ì„ ë‹¤í˜•", "true_answer":..., "question":..., 
                    "rich_pred":..., "format_aware_pred":..., ... }, â€¦ ],
        "ë‹¨ë‹µí˜•": [ â€¦ ],
        "ì„œìˆ í˜•": [ â€¦ ]
        }
        """
        results = {
            'ì„ ë‹¤í˜•': [],
            'ë‹¨ë‹µí˜•': [],
            'ì„œìˆ í˜•': []
        }

        for question_type, samples in data.items():
            for sample in samples:
                rec = sample.copy()  # ì›ë³¸ ë ˆì½”ë“œ ë³µì‚¬

                # `_pred` ë¡œ ëë‚˜ëŠ” í•„ë“œë§Œ ì°¾ì•„ì„œ í‰ê°€
                for key, pred_answer in sample.items():
                    if not key.endswith('_pred'):
                        continue

                    name = key[:-5]  # 'rich_pred' -> 'rich'

                    original_answer = pred_answer
                    if '<answer>' in pred_answer:
                        answer_tag = '<answer>'
                    else:
                        answer_tag = 'ì •ë‹µ:'
                    pred_answer = pred_answer.split(answer_tag)[-1].strip()
                    pred_answer = pred_answer.replace('*', '').replace('</answer>','')

                    if rec['true_answer'] is None:
                        print('# ë¶ˆëŸ‰')
                        print(rec)

                    # ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê°€
                    if question_type == "ì„ ë‹¤í˜•":
                        rec[f'{name}_score'] = self.evaluate_multiple_choice(pred_answer, rec['true_answer'])
                        
                    elif question_type == "ë‹¨ë‹µí˜•":
                        rec[f'{name}_exact'] = self.evaluate_short_answer(pred_answer, rec['true_answer'])
                        
                    else:  # ì„œìˆ í˜•
                        # 1) Rouge
                        rouge = self.evaluate_long_answer(original_answer, rec['true_answer'])
                        rec[f'{name}_rouge1'] = rouge['rouge1']
                        rec[f'{name}_rouge2'] = rouge['rouge2']
                        rec[f'{name}_rougeL'] = rouge['rougeL']

                        # 2) BLEU
                        rec[f'{name}_bleu']   = self.calc_BLEU(original_answer, rec['true_answer'])


                results[question_type].append(rec)

        return results


    def analyze_results(self, results):
        """ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
        analysis = {}
        
        for question_type in ['ì„ ë‹¤í˜•', 'ë‹¨ë‹µí˜•', 'ì„œìˆ í˜•']:
            if not results[question_type]:
                continue
                
            type_data = results[question_type]
            analysis[question_type] = {}
            
            # for prompt_name in ['baseline', 'simple', 'rich', 'expert', 'format_aware', 'detailed']:
            for prompt_name in ['_'.join(x.split('_')[:-1]) for x in type_data[0].keys() if x.endswith('_pred')]:
                if question_type == "ì„ ë‹¤í˜•":
                    print(type_data[0].keys())
                    scores = [item[f'{prompt_name}_score'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'accuracy': np.mean(scores),
                        'count': len(scores)
                    }
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact_scores = [item[f'{prompt_name}_exact'] for item in type_data]
                    # partial_scores = [item[f'{prompt_name}_partial'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'exact_match': np.mean(exact_scores),
                        # 'partial_match': np.mean(partial_scores),
                        'count': len(exact_scores)
                    }
                    
                else:  # ì„œìˆ í˜•
                    rouge1_scores = [item[f'{prompt_name}_rouge1'] for item in type_data]
                    rouge2_scores = [item[f'{prompt_name}_rouge2'] for item in type_data]
                    rougeL_scores = [item[f'{prompt_name}_rougeL'] for item in type_data]
                    bleu_scores     = [item[f'{prompt_name}_bleu']      for item in type_data]
                    # bert_scores     = [item[f'{prompt_name}_bertscore'] for item in type_data]
                    # bleurt_scores   = [item[f'{prompt_name}_bleurt']   for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'rouge1': np.mean(rouge1_scores),
                        'rouge2': np.mean(rouge2_scores),
                        'rougeL': np.mean(rougeL_scores),
                        'bleu': np.mean(bleu_scores),
                        # 'bertscore': np.mean(bert_scores),
                        # 'bleurt': np.mean(bleurt_scores),
                        'count': len(rouge1_scores)
                    }
        
        return analysis
    
    def print_analysis(self, analysis, save_path="analysis_result.md"):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë° .md íŒŒì¼ ì €ì¥"""
        output_lines = []

        def write(line=""):
            print(line)
            output_lines.append(line)

        write("\n" + "="*80)
        write("PHASE 1: PROMPTING EXPERIMENT RESULTS")
        write("="*80)
        
        for question_type, type_results in analysis.items():
            write(f"\nğŸ“Š {question_type} ê²°ê³¼:")
            write("-" * 50)
            
            if question_type == "ì„ ë‹¤í˜•":
                for prompt_name, metrics in type_results.items():
                    write(f"{prompt_name:15}: Accuracy = {metrics['accuracy']:.3f} (n={metrics['count']})")
                    
            elif question_type == "ë‹¨ë‹µí˜•":
                for prompt_name, metrics in type_results.items():
                    write(f"{prompt_name:15}: Exact = {metrics['exact_match']:.3f} (n={metrics['count']})")
                    
            else:  # ì„œìˆ í˜•
                for prompt_name, metrics in type_results.items():
                    write(
                        f"{prompt_name:15}: "
                        f"ROUGE-1 = {metrics['rouge1']:.3f}, "
                        f"ROUGE-2 = {metrics['rouge2']:.3f}, "
                        f"ROUGE-L = {metrics['rougeL']:.3f}, "
                        f"BLEU = {metrics['bleu']:.3f} "
                        f"(n={metrics['count']})"
                    )

        write(f"\nğŸ† ì¢…í•© ìˆœìœ„:")
        write("-" * 30)

        prompt_scores = {}
        for prompt_name in analysis['ì„ ë‹¤í˜•'].keys():
            total_score = 0
            total_count = 0
            
            for question_type, type_results in analysis.items():
                if prompt_name in type_results:
                    if question_type == "ì„ ë‹¤í˜•":
                        score = type_results[prompt_name]['accuracy']
                    elif question_type == "ë‹¨ë‹µí˜•":
                        score = type_results[prompt_name]['exact_match']
                    else:  # ì„œìˆ í˜•
                        score = type_results[prompt_name]['rouge1']
                    
                    count = type_results[prompt_name]['count']
                    total_score += score * count
                    total_count += count
            
            if total_count > 0:
                prompt_scores[prompt_name] = total_score / total_count

        # ìˆœìœ„ ì •ë ¬ ë° ì¶œë ¥
        ranked_prompts = sorted(prompt_scores.items(), key=lambda x: x[1], reverse=True)
        for i, (prompt_name, score) in enumerate(ranked_prompts, 1):
            write(f"{i}. {prompt_name:15}: {score:.3f}")
        
        # .md íŒŒì¼ ì €ì¥
        with open(save_path, "w", encoding="utf-8") as f:
            f.write("\n".join(output_lines))


    def save_final_results(self, results, analysis):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        # ìƒì„¸ ê²°ê³¼
        with open(f'results/phase1_{"_".join(self.model_name.split("/")[-2:])}_detailed_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ë¶„ì„ ìš”ì•½
        with open(f'results/phase1_{"_".join(self.model_name.split("/")[-2:])}_analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - phase1_{'_'.join(self.model_name.split('/')[-2:])}_detailed_results.json")
        print(f"   - phase1_{'_'.join(self.model_name.split('/')[-2:])}_analysis_summary.json")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 1")
    print("="*60)
    
    # ì‹¤í—˜ ì´ˆê¸°í™”
    experiment = PromptingExperiment()
    
    # ë°ì´í„° ë¡œë“œ
    train_data, dev_data, test_data = experiment.load_data()
    
    # Dev setìœ¼ë¡œ ì‹¤í—˜ (ì‹œê°„ ì ˆì•½ì„ ìœ„í•´)
    print(f"\nğŸ”¬ Dev setìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘ (n={len(dev_data)})")
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = experiment.run_experiment(dev_data, sample_size=None)
    
    # ê²°ê³¼ ë¶„ì„
    analysis = experiment.analyze_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    experiment.print_analysis(analysis)
    
    # ê²°ê³¼ ì €ì¥
    experiment.save_final_results(results, analysis)
    
    print(f"\nâœ… Phase 1 ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 