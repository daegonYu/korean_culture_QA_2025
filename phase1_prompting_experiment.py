import json
import pandas as pd
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import re
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
from rouge_score import rouge_scorer
import warnings
warnings.filterwarnings('ignore')

class PromptingExperiment:
    def __init__(self, model_name="beomi/Kanana-8B"):
        """
        Phase 1: í”„ë¡¬í”„íŒ… ì‹¤í—˜
        5ê°€ì§€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ Kanana 8B ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        """
        print(f"Loading {model_name}...")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"Model loaded on {self.device}")
        
    def load_data(self, data_path="data"):
        """ë°ì´í„° ë¡œë“œ"""
        data_dir = Path(data_path)
        
        with open(data_dir / "train.json", "r", encoding="utf-8") as f:
            train_data = json.load(f)
        with open(data_dir / "dev.json", "r", encoding="utf-8") as f:
            dev_data = json.load(f)
            
        print(f"Loaded {len(train_data)} train samples, {len(dev_data)} dev samples")
        return train_data, dev_data
    
    def create_prompts(self, sample):
        """5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question = sample['input']['question']
        question_type = sample['input']['question_type']
        category = sample['input']['category']
        domain = sample['input']['domain']
        topic_keyword = sample['input']['topic_keyword']
        
        prompts = {}
        
        # 1. Baseline: questionë§Œ
        prompts['baseline'] = f"ì§ˆë¬¸: {question}\në‹µë³€:"
        
        # 2. Simple: question_type + question
        prompts['simple'] = f"[{question_type}] {question}\në‹µë³€:"
        
        # 3. Rich: ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨
        prompts['rich'] = f"[ë¶„ë¥˜: {category}] [ë„ë©”ì¸: {domain}] [ìœ í˜•: {question_type}] [ì£¼ì œ: {topic_keyword}]\nì§ˆë¬¸: {question}\në‹µë³€:"
        
        # 4. Expert: ì „ë¬¸ê°€ ì—­í•  ë¶€ì—¬
        prompts['expert'] = f"ë‹¹ì‹ ì€ í•œêµ­ ë¬¸í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì ì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {question}\në‹µë³€:"
        
        # 5. Format-aware: ë‹µë³€ í˜•ì‹ ëª…ì‹œ
        if question_type == "ì„ ë‹¤í˜•":
            format_instruction = "ì •ë‹µ ë²ˆí˜¸ë§Œ ì…ë ¥í•˜ì„¸ìš”."
        elif question_type == "ë‹¨ë‹µí˜•":
            format_instruction = "ê°„ë‹¨í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”."
        else:  # ì„œìˆ í˜•
            format_instruction = "ìì„¸í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."
            
        prompts['format_aware'] = f"[{question_type}] {question}\n\n{format_instruction}\në‹µë³€:"
        
        return prompts
    
    def generate_answer(self, prompt, max_length=512):
        """ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024
            ).to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°)
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            # ë‹µë³€ ì •ë¦¬
            answer = generated_text.strip()
            if '\n' in answer:
                answer = answer.split('\n')[0].strip()
                
            return answer
            
        except Exception as e:
            print(f"Generation error: {e}")
            return ""
    
    def evaluate_multiple_choice(self, pred_answer, true_answer):
        """ì„ ë‹¤í˜• í‰ê°€"""
        # ìˆ«ì ì¶”ì¶œ
        pred_nums = re.findall(r'\b[1-5]\b', pred_answer)
        if pred_nums:
            pred = pred_nums[0]
        else:
            pred = pred_answer.strip()
        
        return 1 if pred == true_answer else 0
    
    def evaluate_short_answer(self, pred_answer, true_answer):
        """ë‹¨ë‹µí˜• í‰ê°€"""
        pred_clean = re.sub(r'[^\wê°€-í£]', '', pred_answer.lower())
        true_clean = re.sub(r'[^\wê°€-í£]', '', true_answer.lower())
        
        # Exact match
        exact_match = 1 if pred_clean == true_clean else 0
        
        # Partial match (í¬í•¨ ê´€ê³„)
        partial_match = 1 if true_clean in pred_clean or pred_clean in true_clean else 0
        
        return exact_match, partial_match
    
    def evaluate_long_answer(self, pred_answer, true_answer):
        """ì„œìˆ í˜• í‰ê°€ (ROUGE ì‚¬ìš©)"""
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
        scores = scorer.score(true_answer, pred_answer)
        
        return {
            'rouge1': scores['rouge1'].fmeasure,
            'rouge2': scores['rouge2'].fmeasure,
            'rougeL': scores['rougeL'].fmeasure
        }
    
    def run_experiment(self, data, sample_size=None, save_results=True):
        """ì‹¤í—˜ ì‹¤í–‰"""
        if sample_size:
            data = data[:sample_size]
            print(f"Using {sample_size} samples for testing")
        
        results = {
            'ì„ ë‹¤í˜•': [],
            'ë‹¨ë‹µí˜•': [],
            'ì„œìˆ í˜•': []
        }
        
        for i, sample in enumerate(tqdm(data, desc="Running experiments")):
            question_type = sample['input']['question_type']
            true_answer = sample['output']['answer']
            
            # 5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = self.create_prompts(sample)
            
            sample_results = {
                'id': sample['id'],
                'question_type': question_type,
                'true_answer': true_answer,
                'question': sample['input']['question']
            }
            
            # ê° í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìƒì„± ë° í‰ê°€
            for prompt_name, prompt in prompts.items():
                pred_answer = self.generate_answer(prompt)
                
                # ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê°€
                if question_type == "ì„ ë‹¤í˜•":
                    score = self.evaluate_multiple_choice(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_pred'] = pred_answer
                    sample_results[f'{prompt_name}_score'] = score
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact, partial = self.evaluate_short_answer(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_pred'] = pred_answer
                    sample_results[f'{prompt_name}_exact'] = exact
                    sample_results[f'{prompt_name}_partial'] = partial
                    
                else:  # ì„œìˆ í˜•
                    rouge_scores = self.evaluate_long_answer(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_pred'] = pred_answer
                    sample_results[f'{prompt_name}_rouge1'] = rouge_scores['rouge1']
                    sample_results[f'{prompt_name}_rouge2'] = rouge_scores['rouge2']
                    sample_results[f'{prompt_name}_rougeL'] = rouge_scores['rougeL']
            
            results[question_type].append(sample_results)
            
            # ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0 and save_results:
                self.save_intermediate_results(results, i + 1)
        
        return results
    
    def save_intermediate_results(self, results, current_idx):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        save_path = f"phase1_intermediate_results_{current_idx}.json"
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def analyze_results(self, results):
        """ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
        analysis = {}
        
        for question_type in ['ì„ ë‹¤í˜•', 'ë‹¨ë‹µí˜•', 'ì„œìˆ í˜•']:
            if not results[question_type]:
                continue
                
            type_data = results[question_type]
            analysis[question_type] = {}
            
            for prompt_name in ['baseline', 'simple', 'rich', 'expert', 'format_aware']:
                if question_type == "ì„ ë‹¤í˜•":
                    scores = [item[f'{prompt_name}_score'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'accuracy': np.mean(scores),
                        'count': len(scores)
                    }
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact_scores = [item[f'{prompt_name}_exact'] for item in type_data]
                    partial_scores = [item[f'{prompt_name}_partial'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'exact_match': np.mean(exact_scores),
                        'partial_match': np.mean(partial_scores),
                        'count': len(exact_scores)
                    }
                    
                else:  # ì„œìˆ í˜•
                    rouge1_scores = [item[f'{prompt_name}_rouge1'] for item in type_data]
                    rouge2_scores = [item[f'{prompt_name}_rouge2'] for item in type_data]
                    rougeL_scores = [item[f'{prompt_name}_rougeL'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'rouge1': np.mean(rouge1_scores),
                        'rouge2': np.mean(rouge2_scores),
                        'rougeL': np.mean(rougeL_scores),
                        'count': len(rouge1_scores)
                    }
        
        return analysis
    
    def print_analysis(self, analysis):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("PHASE 1: PROMPTING EXPERIMENT RESULTS")
        print("="*80)
        
        for question_type, type_results in analysis.items():
            print(f"\nğŸ“Š {question_type} ê²°ê³¼:")
            print("-" * 50)
            
            if question_type == "ì„ ë‹¤í˜•":
                for prompt_name, metrics in type_results.items():
                    print(f"{prompt_name:15}: Accuracy = {metrics['accuracy']:.3f} (n={metrics['count']})")
                    
            elif question_type == "ë‹¨ë‹µí˜•":
                for prompt_name, metrics in type_results.items():
                    print(f"{prompt_name:15}: Exact = {metrics['exact_match']:.3f}, Partial = {metrics['partial_match']:.3f} (n={metrics['count']})")
                    
            else:  # ì„œìˆ í˜•
                for prompt_name, metrics in type_results.items():
                    print(f"{prompt_name:15}: ROUGE-1 = {metrics['rouge1']:.3f}, ROUGE-L = {metrics['rougeL']:.3f} (n={metrics['count']})")
        
        # ì „ì²´ í‰ê·  (ê°€ì¤‘ í‰ê· )
        print(f"\nğŸ† ì¢…í•© ìˆœìœ„:")
        print("-" * 30)
        
        prompt_scores = {}
        for prompt_name in ['baseline', 'simple', 'rich', 'expert', 'format_aware']:
            total_score = 0
            total_count = 0
            
            for question_type, type_results in analysis.items():
                if prompt_name in type_results:
                    if question_type == "ì„ ë‹¤í˜•":
                        score = type_results[prompt_name]['accuracy']
                    elif question_type == "ë‹¨ë‹µí˜•":
                        score = type_results[prompt_name]['exact_match']
                    else:  # ì„œìˆ í˜•
                        score = type_results[prompt_name]['rouge1']
                    
                    count = type_results[prompt_name]['count']
                    total_score += score * count
                    total_count += count
            
            if total_count > 0:
                prompt_scores[prompt_name] = total_score / total_count
        
        # ìˆœìœ„ ì •ë ¬
        ranked_prompts = sorted(prompt_scores.items(), key=lambda x: x[1], reverse=True)
        for i, (prompt_name, score) in enumerate(ranked_prompts, 1):
            print(f"{i}. {prompt_name:15}: {score:.3f}")
    
    def save_final_results(self, results, analysis):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        # ìƒì„¸ ê²°ê³¼
        with open('phase1_detailed_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ë¶„ì„ ìš”ì•½
        with open('phase1_analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - phase1_detailed_results.json")
        print(f"   - phase1_analysis_summary.json")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 1: Prompting Experiment with Kanana 8B")
    print("="*60)
    
    # ì‹¤í—˜ ì´ˆê¸°í™”
    experiment = PromptingExperiment("beomi/Kanana-8B")
    
    # ë°ì´í„° ë¡œë“œ
    train_data, dev_data = experiment.load_data()
    
    # Dev setìœ¼ë¡œ ì‹¤í—˜ (ì‹œê°„ ì ˆì•½ì„ ìœ„í•´)
    print(f"\nğŸ”¬ Dev setìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘ (n={len(dev_data)})")
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = experiment.run_experiment(dev_data, sample_size=None)
    
    # ê²°ê³¼ ë¶„ì„
    analysis = experiment.analyze_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    experiment.print_analysis(analysis)
    
    # ê²°ê³¼ ì €ì¥
    experiment.save_final_results(results, analysis)
    
    print(f"\nâœ… Phase 1 ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 