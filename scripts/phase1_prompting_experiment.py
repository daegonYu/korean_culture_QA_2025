import argparse
import json
import os
import re
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from datasets import Dataset
from dotenv import load_dotenv
from peft import PeftConfig
from rouge_score import rouge_scorer
from sklearn.metrics import accuracy_score, f1_score
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer
from unsloth import FastLanguageModel
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
import wandb


warnings.filterwarnings('ignore')
load_dotenv()


class PromptingExperiment:
    def __init__(self, model_name="beomi/Kanana-8B", load_model=True, use_lora=False, use_wandb=False, \
                system_prompt="", system_prompt2="", user_prompt="", user_prompt2="", answer_tag="ì •ë‹µ:", max_lora_rank=64):
        """
        Phase 1: í”„ë¡¬í”„íŒ… ì‹¤í—˜
        5ê°€ì§€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ Kanana 8B ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        """

        self.model_name = model_name
        self.system_prompt = system_prompt
        self.system_prompt2 = system_prompt2        # ì„œìˆ í˜• ìœ í˜•ì— ëŒ€í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„œìˆ í˜•ì˜ ê²½ìš° ë”°ë¡œ í”„ë¡¬í”„íŒ…)
        self.user_prompt = user_prompt
        self.user_prompt2 = user_prompt2            # ì„œìˆ í˜• ìœ í˜•ì— ëŒ€í•œ ìœ ì € í”„ë¡¬í”„íŠ¸
        self.answer_tag = answer_tag
        self.use_lora = use_lora
        self.use_wandb = use_wandb
        self.max_lora_rank = max_lora_rank
        # wandb ì´ˆê¸°í™”
        if self.use_wandb:
            load_dotenv()
            wandb_api_key = os.getenv("WANDB_API_KEY")

            wandb.login(key=wandb_api_key)

            wandb.init(
                project="moducorpus_korea_culture_answer_log",
                name=f"{model_name.replace('/', '_')}",
                config={
                    "model_name": model_name,
                    "load_model": load_model,
                    "use_lora": self.use_lora,
                    "system_prompt": system_prompt,
                    "system_prompt_ì„œìˆ í˜•": system_prompt2,
                    "user_prompt_template": user_prompt,
                    "user_prompt_template_ì„œìˆ í˜•": user_prompt2,
                    "answer_tag": answer_tag
                }
            )
            self.wb_table = wandb.Table(columns=["prompt_system", "prompt_user", "answer"])

        self.llm = None
        self.lora_req = None
        max_model_len=3096
        if load_model and not self.use_lora:
            print(f"Loading {self.model_name}...")
            self.llm = LLM(
                model=self.model_name,
                dtype="bfloat16",     # ë˜ëŠ” "float16", "auto"
                trust_remote_code=True,
                max_model_len=max_model_len,   # ìµœëŒ€ ì…ë ¥ ê¸¸ì´
                gpu_memory_utilization=0.9,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            )
        elif load_model and self.use_lora:
            config = PeftConfig.from_pretrained(self.model_name)
            base_model_name = config.base_model_name_or_path
            print(f"Loading {self.model_name} with LoRA Layers...")
            self.llm = LLM(
                model=base_model_name,
                dtype="bfloat16",     # ë˜ëŠ” "float16", "auto"
                trust_remote_code=True,
                max_model_len=max_model_len,   # ìµœëŒ€ ì…ë ¥ ê¸¸ì´
                gpu_memory_utilization=0.9,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                enable_lora=self.use_lora,
                max_lora_rank=self.max_lora_rank
            )
            self.lora_req = LoRARequest("adapter", 1, self.model_name)

        # Sampling íŒŒë¼ë¯¸í„° ì„¤ì • (ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ì¤€ë¹„)
        self.sampling_params = SamplingParams(
            max_tokens=2000,
            temperature=0.6,
            top_p=0.95,
            # best_of=8,
            # n=1
        )
                
    def load_data(self, data_path="data"):
        """ë°ì´í„° ë¡œë“œ"""
        data_dir = Path(data_path)
        
        with open(data_dir / "train.json", "r", encoding="utf-8") as f:
            train_data = json.load(f)
        with open(data_dir / "dev.json", "r", encoding="utf-8") as f:
            dev_data = json.load(f)
        with open(data_dir / "preprocessed/test.json", "r", encoding="utf-8") as f:
            test_data = json.load(f)
        
        print(f"Loaded {len(train_data)} train, {len(dev_data)} dev" + (f", {len(test_data)} test" if test_data else ""))
        return train_data, dev_data, test_data
    
    def create_prompts(self, sample):
        """5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question = sample['input']['question']
        question_type = sample['input']['question_type']
        category = sample['input']['category']
        domain = sample['input']['domain']
        topic_keyword = sample['input']['topic_keyword']
        
        prompts = {}

        if question_type != 'ì„œìˆ í˜•':
            system_prompt = self.system_prompt
            user_prompt = self.user_prompt
            user_prompt = user_prompt.format(
                category=category,
                domain=domain,
                topic_keyword=topic_keyword,
                question_type=question_type,
                question=question
            )
        else:
            system_prompt = self.system_prompt2
            user_prompt = self.user_prompt2
            user_prompt = user_prompt.format(
                category=category,
                domain=domain,
                topic_keyword=topic_keyword,
                question_type=question_type,
                question=question
            )
        print(f"system_prompt:{system_prompt}")    
        print(f"user_prompt:{user_prompt}")

        prompts['experiment'] = {"system_prompt":system_prompt, "user_prompt": user_prompt}
        
        return prompts
    
    def generate_answer(self, prompt):
        """ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            messages = [
                {"role": "system", "content": prompt['system_prompt']},
                {"role": "user", "content": prompt['user_prompt']}
            ]

            # ìƒì„±
            if self.use_lora:
                outputs = self.llm.chat(
                    messages=messages,
                    sampling_params=self.sampling_params,
                    lora_request=self.lora_req
                )
            else:
                outputs = self.llm.chat(
                    messages=messages,
                    sampling_params=self.sampling_params,
                )

            # ë””ì½”ë”© (ì…ë ¥ ê¸¸ì´ ì´í›„ë§Œ ì¶”ì¶œ)
            generated_text = outputs[0].outputs[0].text
            
            # ë‹µë³€ ì •ë¦¬
            answer = generated_text.strip()

            # wandb ë¡œê·¸ ì¶”ê°€
            if self.use_wandb:
                self.wb_table.add_data(
                    prompt['system_prompt'],
                    prompt['user_prompt'],
                    answer
                )

            print(answer)
            return answer
            
        except Exception as e:
            print(f"Generation error: {e}")
            return ""
    
    def evaluate_multiple_choice(self, pred_answer, true_answer):
        """ì„ ë‹¤í˜• í‰ê°€"""
        # ìˆ«ì ì¶”ì¶œ
        # pred_nums = re.findall(r'\b[1-5]\b', pred_answer)
        pred_nums = re.findall(r'[1-5]', pred_answer)
        if pred_nums:
            pred = pred_nums[0]
        else:
            pred = pred_answer.strip()
        
        return 1 if pred == true_answer else 0
    
    def evaluate_short_answer(self, pred_answer, true_answer):
        """
        Calculate Exact Match score where true_data may contain multiple acceptable answers separated by #
        """
        correct = 0
        true_answer_list = true_answer.split('#')

        # if any(pred_answer.replace(' ','') == ans.replace(' ','') for ans in true_answer_list):
        if any(pred_answer == true_answer for true_answer in true_answer_list):
            correct = 1
                
        return correct
    
    def evaluate_long_answer(self, pred_answer, true_answer):
        """ì„œìˆ í˜• í‰ê°€ (ROUGE-1, ROUGE-2, ROUGE-L via rouge_metric)"""
        from rouge_metric import Rouge

        rouge_evaluator = Rouge(
            metrics=["rouge-n", "rouge-l"],
            max_n=2,
            limit_length=True,
            length_limit=1000,
            length_limit_type="words",
            use_tokenizer=True,
            apply_avg=True,
            apply_best=False,
            alpha=0.5,        # F1 score
            weight_factor=1.0,
        )

        # ë‹¨ì¼ ë¬¸ì¥ pairë„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
        scores = rouge_evaluator.get_scores(
            [pred_answer],
            [true_answer]
        )

        return {
            'rouge1':   scores['rouge-1']['f'],
            'rouge2':   scores['rouge-2']['f'],
            'rougeL':   scores['rouge-l']['f']
        }

    def calc_BLEU(self, pred_answer: str, true_answer: str, apply_avg=True, apply_best=False, use_mecab=True):
        from nltk.translate.bleu_score import sentence_bleu
        from konlpy.tag import Mecab

        tokenizer = Mecab()
        stacked_bleu = []

        # 1) ë‹¨ì¼ ë¬¸ìì—´ì„ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
        refs = [true_answer]  # ë¦¬ìŠ¤íŠ¸ of reference strings
        cand = pred_answer    # í•˜ë‚˜ì˜ candidate string

        # 2) í† í¬ë‚˜ì´ì§•
        if use_mecab:
            cand_tokens = tokenizer.morphs(cand)
        else:
            cand_tokens = cand.split()

        # 3) ê° ref í…ìŠ¤íŠ¸ë³„ë¡œ BLEU ê³„ì‚°
        best_bleu = 0
        sum_bleu  = 0
        for ref_text in refs:
            if use_mecab:
                ref_tokens = tokenizer.morphs(ref_text)
            else:
                ref_tokens = ref_text.split()

            score = sentence_bleu([ref_tokens], cand_tokens, weights=(1, 0, 0, 0))
            sum_bleu  += score
            best_bleu = max(best_bleu, score)

        # 4) í‰ê· 
        avg_bleu = sum_bleu / len(refs)  # ì´ì œ len(refs) == 1

        # 5) ê²°ê³¼ ìˆ˜ì§‘
        if apply_best:
            stacked_bleu.append(best_bleu)
        if apply_avg:
            stacked_bleu.append(avg_bleu)

        return sum(stacked_bleu) / len(stacked_bleu) if stacked_bleu else 0.0


    def run_experiment(self, data, sample_size=None, save_results=True, test_mode=False):
        """ì‹¤í—˜ ì‹¤í–‰"""
        if sample_size:
            data = data[:sample_size]
            print(f"Using {sample_size} samples for testing")
        
        results = {
            'ì„ ë‹¤í˜•': [],
            'ë‹¨ë‹µí˜•': [],
            'ì„œìˆ í˜•': []
        }
        
        for i, sample in enumerate(tqdm(data, desc="Running experiments")):
            question_type = sample['input']['question_type']
            true_answer = sample['output']['answer'] if 'output' in sample else None
            
            # 5ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = self.create_prompts(sample)
            
            sample_results = {
                'id': sample['id'],
                'question_type': question_type,
                'true_answer': true_answer,
                'question': sample['input']['question']
            }
            
            # ê° í”„ë¡¬í”„íŠ¸ë¡œ ë‹µë³€ ìƒì„± ë° í‰ê°€
            for prompt_name, prompt in prompts.items():
                pred_answer = self.generate_answer(prompt=prompt)
                sample_results[f'{prompt_name}_pred'] = pred_answer
                if test_mode:
                    continue  # í‰ê°€ ìŠ¤í‚µ

                original_answer = pred_answer
                if self.answer_tag != '' and self.answer_tag in pred_answer:
                    pred_answer = pred_answer.split(self.answer_tag)[-1].strip()
                pred_answer = pred_answer.replace('*', '').replace('</answer>','')
                
                # ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê°€
                if question_type == "ì„ ë‹¤í˜•":
                    score = self.evaluate_multiple_choice(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_score'] = score
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact = self.evaluate_short_answer(pred_answer, true_answer)
                    sample_results[f'{prompt_name}_exact'] = exact
                    
                else:  # ì„œìˆ í˜•
                    # 1) Rouge
                    rouge_scores = self.evaluate_long_answer(original_answer, true_answer)
                    sample_results[f'{prompt_name}_rouge1'] = rouge_scores['rouge1']
                    sample_results[f'{prompt_name}_rouge2'] = rouge_scores['rouge2']
                    sample_results[f'{prompt_name}_rougeL'] = rouge_scores['rougeL']

                    # 2) BLEU
                    bleu_score = self.calc_BLEU(original_answer, true_answer)
                    sample_results[f'{prompt_name}_bleu'] = bleu_score
            
            results[question_type].append(sample_results)
            
            # ì¤‘ê°„ ì €ì¥ (10ê°œë§ˆë‹¤)
            # if ((i + 1) % 100 == 0) or (i + 1 == len(data)) and save_results:
            #     self.save_intermediate_results(results, i + 1)
        
        wandb.log({"prompts_and_answers": self.wb_table})
        return results
    
    def save_intermediate_results(self, results, current_idx):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        save_path = f"results/phase1_{self.model_name.split('/')[-1]}_intermediate_results_{current_idx}.json"
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    def only_scoring(self, data: dict) -> dict:
        """
        data: {
        "ì„ ë‹¤í˜•": [ { "id":..., "question_type":"ì„ ë‹¤í˜•", "true_answer":..., "question":..., 
                    "rich_pred":..., "format_aware_pred":..., ... }, â€¦ ],
        "ë‹¨ë‹µí˜•": [ â€¦ ],
        "ì„œìˆ í˜•": [ â€¦ ]
        }
        """
        results = {
            'ì„ ë‹¤í˜•': [],
            'ë‹¨ë‹µí˜•': [],
            'ì„œìˆ í˜•': []
        }

        for question_type, samples in data.items():
            for sample in samples:
                rec = sample.copy()  # ì›ë³¸ ë ˆì½”ë“œ ë³µì‚¬

                # `_pred` ë¡œ ëë‚˜ëŠ” í•„ë“œë§Œ ì°¾ì•„ì„œ í‰ê°€
                for key, pred_answer in sample.items():
                    if not key.endswith('_pred'):
                        continue

                    name = key[:-5]  # 'rich_pred' -> 'rich'

                    original_answer = pred_answer
                    if '<answer>' in pred_answer:
                        answer_tag = '<answer>'
                    else:
                        answer_tag = 'ì •ë‹µ:'
                    pred_answer = pred_answer.split(answer_tag)[-1].strip()
                    pred_answer = pred_answer.replace('*', '').replace('</answer>','')

                    if rec['true_answer'] is None:
                        print('# ë¶ˆëŸ‰')
                        print(rec)

                    # ì§ˆë¬¸ ìœ í˜•ë³„ í‰ê°€
                    if question_type == "ì„ ë‹¤í˜•":
                        rec[f'{name}_score'] = self.evaluate_multiple_choice(pred_answer, rec['true_answer'])
                        
                    elif question_type == "ë‹¨ë‹µí˜•":
                        rec[f'{name}_exact'] = self.evaluate_short_answer(pred_answer, rec['true_answer'])
                        
                    else:  # ì„œìˆ í˜•
                        # 1) Rouge
                        rouge = self.evaluate_long_answer(original_answer, rec['true_answer'])
                        rec[f'{name}_rouge1'] = rouge['rouge1']
                        rec[f'{name}_rouge2'] = rouge['rouge2']
                        rec[f'{name}_rougeL'] = rouge['rougeL']

                        # 2) BLEU
                        rec[f'{name}_bleu']   = self.calc_BLEU(original_answer, rec['true_answer'])


                results[question_type].append(rec)

        return results


    def analyze_results(self, results):
        """ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
        analysis = {}
        
        for question_type in ['ì„ ë‹¤í˜•', 'ë‹¨ë‹µí˜•', 'ì„œìˆ í˜•']:
            if not results[question_type]:
                continue
                
            type_data = results[question_type]
            analysis[question_type] = {}
            
            for prompt_name in ['_'.join(x.split('_')[:-1]) for x in type_data[0].keys() if x.endswith('_pred')]:
                if question_type == "ì„ ë‹¤í˜•":
                    print(type_data[0].keys())
                    scores = [item[f'{prompt_name}_score'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'accuracy': np.mean(scores),
                        'count': len(scores)
                    }
                    
                elif question_type == "ë‹¨ë‹µí˜•":
                    exact_scores = [item[f'{prompt_name}_exact'] for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'exact_match': np.mean(exact_scores),
                        'count': len(exact_scores)
                    }
                    
                else:  # ì„œìˆ í˜•
                    rouge1_scores = [item[f'{prompt_name}_rouge1'] for item in type_data]
                    rouge2_scores = [item[f'{prompt_name}_rouge2'] for item in type_data]
                    rougeL_scores = [item[f'{prompt_name}_rougeL'] for item in type_data]
                    bleu_scores     = [item[f'{prompt_name}_bleu']      for item in type_data]
                    analysis[question_type][prompt_name] = {
                        'rouge1': np.mean(rouge1_scores),
                        'rouge2': np.mean(rouge2_scores),
                        'rougeL': np.mean(rougeL_scores),
                        'bleu': np.mean(bleu_scores),
                        'count': len(rouge1_scores)
                    }
        
        return analysis
    
    def print_analysis(self, analysis, save_path="analysis_result.md"):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë° .md íŒŒì¼ ì €ì¥"""
        output_lines = []

        def write(line=""):
            print(line)
            output_lines.append(line)

        write("\n" + "="*80)
        write("PHASE 1: PROMPTING EXPERIMENT RESULTS")
        write("="*80)
        
        for question_type, type_results in analysis.items():
            write(f"\nğŸ“Š {question_type} ê²°ê³¼:")
            write("-" * 50)
            
            if question_type == "ì„ ë‹¤í˜•":
                for prompt_name, metrics in type_results.items():
                    write(f"{prompt_name:15}: Accuracy = {metrics['accuracy']:.3f} (n={metrics['count']})")
                    
            elif question_type == "ë‹¨ë‹µí˜•":
                for prompt_name, metrics in type_results.items():
                    write(f"{prompt_name:15}: Exact = {metrics['exact_match']:.3f} (n={metrics['count']})")
                    
            else:  # ì„œìˆ í˜•
                for prompt_name, metrics in type_results.items():
                    write(
                        f"{prompt_name:15}: "
                        f"ROUGE-1 = {metrics['rouge1']:.3f}, "
                        f"ROUGE-2 = {metrics['rouge2']:.3f}, "
                        f"ROUGE-L = {metrics['rougeL']:.3f}, "
                        f"BLEU = {metrics['bleu']:.3f} "
                        f"(n={metrics['count']})"
                    )

        write(f"\nğŸ† ì¢…í•© ìˆœìœ„:")
        write("-" * 30)

        prompt_scores = {}
        for prompt_name in analysis['ì„ ë‹¤í˜•'].keys():
            total_score = 0
            total_count = 0
            
            for question_type, type_results in analysis.items():
                if prompt_name in type_results:
                    if question_type == "ì„ ë‹¤í˜•":
                        score = type_results[prompt_name]['accuracy']
                    elif question_type == "ë‹¨ë‹µí˜•":
                        score = type_results[prompt_name]['exact_match']
                    else:  # ì„œìˆ í˜•
                        score = type_results[prompt_name]['rouge1']
                    
                    count = type_results[prompt_name]['count']
                    total_score += score * count
                    total_count += count
            
            if total_count > 0:
                prompt_scores[prompt_name] = total_score / total_count

        # ìˆœìœ„ ì •ë ¬ ë° ì¶œë ¥
        ranked_prompts = sorted(prompt_scores.items(), key=lambda x: x[1], reverse=True)
        for i, (prompt_name, score) in enumerate(ranked_prompts, 1):
            write(f"{i}. {prompt_name:15}: {score:.3f}")
        
        # .md íŒŒì¼ ì €ì¥
        with open(save_path, "w", encoding="utf-8") as f:
            f.write("\n".join(output_lines))


    def save_final_results(self, results, analysis):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        # ìƒì„¸ ê²°ê³¼
        with open(f'results/phase1_{"_".join(self.model_name.split("/")[-2:])}_detailed_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ë¶„ì„ ìš”ì•½
        with open(f'results/phase1_{"_".join(self.model_name.split("/")[-2:])}_analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - phase1_{'_'.join(self.model_name.split('/')[-2:])}_detailed_results.json")
        print(f"   - phase1_{'_'.join(self.model_name.split('/')[-2:])}_analysis_summary.json")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 1")
    print("="*60)
    
    # ì‹¤í—˜ ì´ˆê¸°í™”
    experiment = PromptingExperiment()
    
    # ë°ì´í„° ë¡œë“œ
    train_data, dev_data, test_data = experiment.load_data()
    
    # Dev setìœ¼ë¡œ ì‹¤í—˜ (ì‹œê°„ ì ˆì•½ì„ ìœ„í•´)
    print(f"\nğŸ”¬ Dev setìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘ (n={len(dev_data)})")
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = experiment.run_experiment(dev_data, sample_size=None)
    
    # ê²°ê³¼ ë¶„ì„
    analysis = experiment.analyze_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    experiment.print_analysis(analysis)
    
    # ê²°ê³¼ ì €ì¥
    experiment.save_final_results(results, analysis)
    
    print(f"\nâœ… Phase 1 ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 