
from phase1_prompting_experiment import PromptingExperiment


def main():
    parser = argparse.ArgumentParser(description="Phase 1: Prompting Experiment")
    parser.add_argument("--model", default="kakaocorp/kanana-1.5-8b-instruct-2505", help="Model name to use")
    parser.add_argument("--data_path", default="data", help="Path to data directory")
    parser.add_argument("--sample_size", type=int, default=None, help="Number of samples to test (default: all)")
    parser.add_argument("--use_train", action="store_true", help="Use train set instead of dev set")
    parser.add_argument("--use_test", action="store_true", help="Use test set instead of dev set")
    parser.add_argument("--use_lora", action="store_true", default=False, help="Use LoRA layers")
    parser.add_argument("--use_wandb", action="store_true", default=False, help="Use WANDB")
    parser.add_argument("--max_lora_rank", type=int, default=64, help="Max LoRA rank (default: 64)")
    parser.add_argument("--system_prompt", type=str, default="ë‹¹ì‹ ì€ í•œêµ­ ë¬¸í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.")
    parser.add_argument("--user_prompt", type=str, required=True)
    parser.add_argument("--system_prompt2", type=str, required=False, type=str, default=None, help="(Optional) Separate system prompt for descriptive (ì„œìˆ í˜•) question type")
    parser.add_argument("--user_prompt2", type=str, required=False, type=str, default=None, help="(Optional) Separate user prompt for descriptive (ì„œìˆ í˜•) question type")
    parser.add_argument("--answer_tag", type=str, default="ì •ë‹µ:")

    args = parser.parse_args()
    
    print(f"ğŸš€ Phase 1 ì‹¤í—˜ ì‹œì‘")
    print(f"   ëª¨ë¸: {args.model}")
    print("="*60)
    
    if args.system_prompt2 is None:
        args.system_prompt2 = args.system_prompt

    if args.user_prompt2 is None:
        args.user_prompt2 = args.user_prompt

    experiment = PromptingExperiment(
        model_name=args.model,
        system_prompt=args.system_prompt,
        system_prompt2=args.system_prompt2,
        user_prompt=args.user_prompt,
        user_prompt2=args.user_prompt2,
        answer_tag=args.answer_tag,
        use_lora=args.use_lora,
        max_lora_rank=args.max_lora_rank if args.use_lora else None,  
        use_wandb=args.use_wandb
    )
    experiment.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ë°ì´í„° ë¡œë“œ
    train_data, dev_data, test_data = experiment.load_data(args.data_path)

    if args.use_test:
        data_to_use = test_data
        dataset_name = "test"
        test_mode = True
    elif args.use_train:
        data_to_use = train_data
        dataset_name = "train"
        test_mode = False
    else:
        data_to_use = dev_data
        dataset_name = "dev"
        test_mode = False
    
    print(f"\nğŸ”¬ {dataset_name} setìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘ (n={len(data_to_use)})")
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = experiment.run_experiment(data_to_use, sample_size=args.sample_size, test_mode=test_mode)

    if not test_mode:
        analysis = experiment.analyze_results(results)
        experiment.print_analysis(analysis)
        experiment.save_final_results(results, analysis)
    else:
        save_path = f"results/phase1_{'_'.join(args.model.split('/')[-2:])}_test_outputs.json"
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ì…‹ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    print(f"\nâœ… Phase 1 ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ íŒŒì¼:")
    print(f"   - phase1_detailed_results.json")
    print(f"   - phase1_analysis_summary.json")

if __name__ == "__main__":
    main() 