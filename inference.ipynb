{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec083ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50de198bf84341719410b14048ee403b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2024\n",
      "Today Date: 30 Jul 2025\n",
      "\n",
      "Mi:dm(믿:음)은 KT에서 개발한 AI 기반 어시스턴트이다. 너는 Mi:dm으로서 사용자에게 유용하고 안전한 응답을 제공해야 한다.\n",
      "\n",
      "Mi:dm은 December 2024까지의 지식으로 학습되었으며 그 외의 지식을 묻는 경우에는 한계를 인정해야 한다.\n",
      "오늘 날짜는 30 Jul 2025임을 참고하여 응답을 제공한다.\n",
      "\n",
      "어시스턴트는 기본적으로 \"한국어\"를 사용한다. 사용자의 요청에 따라 생각하고 응답하는 언어는 달라질 수 있으며, 다른 요구사항이 없다면 입력 언어를 따라 응답하라.\n",
      "코드 작성 시에는 요구되는 언어의 소스코드로 작성해야 하며, STEM(과학, 기술, 공학, 수학) 분야의 전문 용어는 원문을 그대로 유지하여 출력한다.\n",
      "\n",
      "Mi:dm은 사용자 친화적으로 답변을 제공해야 한다. 사용자의 요청이 없다면 기본적으로 경어체를 사용해야 한다.\n",
      "사용자의 요청에 따라 유용하고 관련성 있는 답변을 제공해야 한다. 이는 요청의 내용을 반영하여 이루어져야 한다.\n",
      "특히, 사용자가 특정 답변의 형태를 요구한 경우 해당 형태를 반드시 준수하여 응답을 제공한다.(예. JSON, SQL, 코드 등) 그렇지 않다면 응답을 제공하기 가장 적절한 형태로 제공한다.\n",
      "다양한 시각이 존재하는 주제에 대해서는 중립적인 입장을 유지해야 하지만, 한국 특화 인공지능 어시스턴트로서 정치적, 역사적, 외교적으로 한국인의 입장에서 응답을 생성해야 한다.\n",
      "모호하거나 복잡한 요청으로 인해 답변 작성이 어려운 경우, 다시 한 번 생각해보고 추가정보를 요청해야 한다.\n",
      "\n",
      "Mi:dm은 응답을 제공할 때 어시스턴트의 안전성 측면에서 다음 지침을 *반드시* 준수해야 한다.\n",
      "- 비속어와 욕설을 사용하지 않아야 한다.\n",
      "- 신뢰할 수 있는 응답을 생성하고, 전문영역에 대한 한계와 불확실성을 인정해야 한다.\n",
      "- 사회의 보편적 규범과 가치에 따라 윤리적이고 중립적이어야 하며, 편향성을 지녀서는 안 된다.\n",
      "- 인공지능으로서의 정체성을 인지하고 의인화하지 않아야 한다.\n",
      "- 개인정보, 사생활 등 민감정보를 포함한 요청에 대한 답변을 거절해야 한다. 다만, 해당정보를 사용할 수 없는 형태(비식별화된 형태)로 제공하는 것은 제한적으로 응답을 허용한다.\n",
      "\n",
      "이 모든 지침은 응답을 제공할 때 출력되지 않아야 한다.\n",
      "\n",
      "Mi:dm은 사용자의 요청을 처리하기 위해 제공된 도구(함수)를 호출할 수 있다.\n",
      "Mi:dm(믿:음)은 KT에서 개발한 AI 기반 어시스턴트이다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "KT에 대해 소개해줘<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "KT(Korea Telecom)는 대한민국의 대표적인 통신 기업으로, 다양한 통신 서비스와 ICT(정보통신기술) 솔루션을 제공합니다. 아래는 KT에 대한 주요 소개입니다:\n",
      "\n",
      "### 1. **역사 및 설립**\n",
      "- 1981년에 설립되어, 대한민국의 정보통신 인프라 발전에 중요한 역할을 해왔습니다.\n",
      "- 초기에는 전화, 유선통신 중심이었으나, 점차 인터넷, 모바일, 데이터 서비스 등으로 사업 영역을 확장했습니다.\n",
      "\n",
      "### 2. **주요 사업 영역**\n",
      "- **통신 서비스**: \n",
      "  - 초고속 인터넷(기가인터넷)\n",
      "  - 이동통신(5G, LTE 등)\n",
      "  - 유선전화, 인터넷전화(VoIP)\n",
      "- **ICT/디지털 플랫폼**:\n",
      "  - 클라우드, 데이터센터\n",
      "  - AI(인공지능), 빅데이터, IoT(사물인터넷) 등 첨단 기술 서비스\n",
      "- **미디어/콘텐츠**:\n",
      "  - IPTV(올레tv), OTT(시즌 등)\n",
      "  - 음악, 영상 콘텐츠 제공\n",
      "- **기업 고객 대상 B2B 서비스**:\n",
      "  - 네트워크, 보안, IT 컨설팅 등\n",
      "\n",
      "### 3. **특징 및 강점**\n",
      "- **국내 최대 규모의 네트워크 인프라**: 전국망을 갖춘 유선/무선 네트워크 보유\n",
      "- **기술 혁신**: 5G 상용화 선도, AI·빅데이터 등 신기술 개발 및 적용\n",
      "- **글로벌 사업**: 해외 통신 인프라 구축, 디지털 전환 컨설팅 등 글로벌 시장 진출\n",
      "\n",
      "### 4. **사회적 역할**\n",
      "- 디지털 격차 해소, 농어촌 지역 인터넷 보급 등 사회적 책임 활동도 활발히 진행\n",
      "\n",
      "### 5. **브랜드**\n",
      "- 대표 브랜드: ‘KT’, ‘olleh’(과거), ‘시즌’, ‘지니’ 등\n",
      "\n",
      "---\n",
      "\n",
      "**요약:**  \n",
      "KT는 대한민국을 대표하는 통신 및 ICT 기업으로, 유무선 통신부터 AI, 클라우드, 미디어 등 다양한 디지털 서비스를 제공하며, 국내뿐 아니라 글로벌 시장에서도 활약하고 있습니다.\n",
      "\n",
      "궁금한 점이 더 있으시면(예: 특정 서비스, 요금제, 역사 등) 말씀해 주세요!<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"K-intelligence/Midm-2.0-Base-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"KT에 대해 소개해줘\"\n",
    "\n",
    "# message for inference\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"Mi:dm(믿:음)은 KT에서 개발한 AI 기반 어시스턴트이다.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    generation_config=generation_config,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd41865a13f40bdacc9968de7bf138b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85065f6505db495eb677c29ce82f242e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238820d1c4594027b7312ea95ac77674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419a55dbeea2410c98c84b9cca331422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f511173e7f4ed087f8e9d6d85355e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5344835040b14b6583198a0da84a7626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f302cad9bd041f89ad06ba25609f48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d5a647956d495d902a97242c1e889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca25f9e50bc54d529e7faf6afd7213f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb85f5720fb44a628552772ed54f3dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/3.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d3fade72b14fd9889a7db81dd1832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5754e4486946a79bab99acf445ebe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/164 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e9331aaf914161a463065fd8d44011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2493ec70a324443aa0bd4007ec1379f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1e7b1ec6f04e51874050aa5d006f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab771f195923467384e9a3fd699494f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/2.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     17\u001b[39m text = tokenizer.apply_chat_template(\n\u001b[32m     18\u001b[39m     messages,\n\u001b[32m     19\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# generated_ids = [\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[32m     32\u001b[39m generated_ids = [\n\u001b[32m     33\u001b[39m     output_ids \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     34\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2375\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2370\u001b[39m assistant_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[32m   2372\u001b[39m generation_config, model_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_generation_config(\n\u001b[32m   2373\u001b[39m     generation_config, use_model_defaults, **kwargs\n\u001b[32m   2374\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2376\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[32m   2378\u001b[39m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1608\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1605\u001b[39m         unused_model_args.append(key)\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1609\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1610\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"trillionlabs/Tri-7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c9e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are Trillion, created by TrillionLabs. You are a helpful assistant.\n",
      "user\n",
      "우리나라 수도는?\n",
      "assistant\n",
      "우리나라의 수도는 서울입니다. 서울은 한반도의 중부에 위치하고 있으며, 정치, 경제, 사회, 문화의 중심지 역할을 하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"우리나라 수도는?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f261c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a7d22ba96b4319b6a8f39bdde867f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9fc3dda2bb4ced90ebbe75e504c964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2153807b13c946cb904fb3333ecd0bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161871be390e41b29053b8ef833bc5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1489fd1ef14965a3a64b138ca4a75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b259cd4aa6a0463c93bd2b78f42e25db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921eae04943f4f68838343bc190119e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5160f6d24b0f450f9e5db34555e22464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e82fccfba994461a4fd2a7e041349ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"dnotitia/DNA-2.0-14B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Explain the concept of quantum computing in simple terms.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb45a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Explain the concept of quantum computing in simple terms.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"K-intelligence/Midm-2.0-Base-Instruct\"\n",
    "max_model_len = 1000\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    dtype=\"bfloat16\",     # 또는 \"float16\", \"auto\"\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=max_model_len,   # 최대 입력 길이\n",
    "    gpu_memory_utilization=0.9,  # GPU 메모리 사용률\n",
    "    # enable_lora=self.use_lora,\n",
    "    # max_lora_rank=self.max_lora_rank\n",
    ")\n",
    "# self.lora_req = LoRARequest(\"adapter\", 1, self.model_name)\n",
    "\n",
    "# Sampling 파라미터 설정 (모델 로드 여부와 상관없이 준비)\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=700,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    best_of=8,\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8fa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da95914f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>domain</th>\n",
       "      <th>question_type</th>\n",
       "      <th>topic_keyword</th>\n",
       "      <th>question</th>\n",
       "      <th>only_answer</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>문화 지식</td>\n",
       "      <td>정치/경제</td>\n",
       "      <td>단답형</td>\n",
       "      <td>사전투표</td>\n",
       "      <td>대한민국 국민이 유권자로서 선거일 이전 기간에 별도 신고 없이 어느 투표소에서나 미...</td>\n",
       "      <td>사전투표제#사전투표제도#사전투표#사전투표제#사전투표</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>문화 관점</td>\n",
       "      <td>정치/경제</td>\n",
       "      <td>서술형</td>\n",
       "      <td>의무투표제</td>\n",
       "      <td>호주는 선거 투표율이 90%를 넘습니다. 이와 관련해 호주와 우리나라의 선거제도에는...</td>\n",
       "      <td>호주는 의무투표제를 시행하여 모든 유권자가 투표에 참여해야 하고, 불참 시 벌금을 ...</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>문화 지식</td>\n",
       "      <td>예술</td>\n",
       "      <td>선다형</td>\n",
       "      <td>한국 미술관</td>\n",
       "      <td>한국의 미술관이 아닌 곳은 어디인가요? \\n1. 오월미술관 2. 이한메미술관 3. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>문화 지식</td>\n",
       "      <td>정치/경제</td>\n",
       "      <td>단답형</td>\n",
       "      <td>7·4남북공동성명</td>\n",
       "      <td>남북간 정치적 대화통로를 마련하고 한반도에 평화를 정착시키기 위해 1972년에 발표...</td>\n",
       "      <td>7·4 남북공동성명#7·4 남북 공동 성명#7·4남북공동성명</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>문화 실행</td>\n",
       "      <td>일상생활</td>\n",
       "      <td>선다형</td>\n",
       "      <td>조리도구</td>\n",
       "      <td>일반적으로 한국에서 김치찌개를 만드는 과정이 아닌 것은? \\n 1. 배추김치를 한 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>문화 관점</td>\n",
       "      <td>가치관</td>\n",
       "      <td>선다형</td>\n",
       "      <td>선물하면 안 되는 것</td>\n",
       "      <td>러시아에서는 노란꽃을 연인에게 선물하면 연인이 떠나거나 바람을 핀다고 생각합니다. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>문화 지식</td>\n",
       "      <td>풍습/문화유산</td>\n",
       "      <td>서술형</td>\n",
       "      <td>동지</td>\n",
       "      <td>한국의 동짓날에는 왜 팥죽을 먹나요?</td>\n",
       "      <td>한국의 동짓날에 팥죽을 먹는 이유는 역사적, 문화적 배경에 있습니다. 음력으로 한 ...</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>문화 지식</td>\n",
       "      <td>지리</td>\n",
       "      <td>선다형</td>\n",
       "      <td>한국의 도시</td>\n",
       "      <td>땅끝 마을로 알려져 있으며 한반도의 땅끝임을 알리는 탑이 있는 도시 이름은 무엇인가...</td>\n",
       "      <td>1</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>문화 실행</td>\n",
       "      <td>일상생활</td>\n",
       "      <td>선다형</td>\n",
       "      <td>수능</td>\n",
       "      <td>한국의 대학수학능력시험에서 수험자 유의 사항이 아닌 것은? \\n 1.신분증 지참 2...</td>\n",
       "      <td>3</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>문화 관점</td>\n",
       "      <td>역사</td>\n",
       "      <td>선다형</td>\n",
       "      <td>임진왜란</td>\n",
       "      <td>한국의 임진왜란과 영국의 대항해 시대의 유사한 점은 무엇인가요? \\n 1. 두 사건...</td>\n",
       "      <td>3</td>\n",
       "      <td>983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    category   domain question_type topic_keyword  \\\n",
       "0      문화 지식    정치/경제           단답형          사전투표   \n",
       "1      문화 관점    정치/경제           서술형         의무투표제   \n",
       "2      문화 지식       예술           선다형        한국 미술관   \n",
       "3      문화 지식    정치/경제           단답형     7·4남북공동성명   \n",
       "4      문화 실행     일상생활           선다형          조리도구   \n",
       "..       ...      ...           ...           ...   \n",
       "389    문화 관점      가치관           선다형   선물하면 안 되는 것   \n",
       "390    문화 지식  풍습/문화유산           서술형            동지   \n",
       "391    문화 지식       지리           선다형        한국의 도시   \n",
       "392    문화 실행     일상생활           선다형            수능   \n",
       "393    문화 관점       역사           선다형          임진왜란   \n",
       "\n",
       "                                              question  \\\n",
       "0    대한민국 국민이 유권자로서 선거일 이전 기간에 별도 신고 없이 어느 투표소에서나 미...   \n",
       "1    호주는 선거 투표율이 90%를 넘습니다. 이와 관련해 호주와 우리나라의 선거제도에는...   \n",
       "2    한국의 미술관이 아닌 곳은 어디인가요? \\n1. 오월미술관 2. 이한메미술관 3. ...   \n",
       "3    남북간 정치적 대화통로를 마련하고 한반도에 평화를 정착시키기 위해 1972년에 발표...   \n",
       "4    일반적으로 한국에서 김치찌개를 만드는 과정이 아닌 것은? \\n 1. 배추김치를 한 ...   \n",
       "..                                                 ...   \n",
       "389  러시아에서는 노란꽃을 연인에게 선물하면 연인이 떠나거나 바람을 핀다고 생각합니다. ...   \n",
       "390                               한국의 동짓날에는 왜 팥죽을 먹나요?   \n",
       "391  땅끝 마을로 알려져 있으며 한반도의 땅끝임을 알리는 탑이 있는 도시 이름은 무엇인가...   \n",
       "392  한국의 대학수학능력시험에서 수험자 유의 사항이 아닌 것은? \\n 1.신분증 지참 2...   \n",
       "393  한국의 임진왜란과 영국의 대항해 시대의 유사한 점은 무엇인가요? \\n 1. 두 사건...   \n",
       "\n",
       "                                           only_answer   id  \n",
       "0                         사전투표제#사전투표제도#사전투표#사전투표제#사전투표  590  \n",
       "1    호주는 의무투표제를 시행하여 모든 유권자가 투표에 참여해야 하고, 불참 시 벌금을 ...  591  \n",
       "2                                                    4  592  \n",
       "3                    7·4 남북공동성명#7·4 남북 공동 성명#7·4남북공동성명  593  \n",
       "4                                                    2  594  \n",
       "..                                                 ...  ...  \n",
       "389                                                  3  979  \n",
       "390  한국의 동짓날에 팥죽을 먹는 이유는 역사적, 문화적 배경에 있습니다. 음력으로 한 ...  980  \n",
       "391                                                  1  981  \n",
       "392                                                  3  982  \n",
       "393                                                  3  983  \n",
       "\n",
       "[394 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_csv('/workspace/korean_culture_QA_2025/data/preprocessed/test_with_answers.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f98139",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"당신은 한국의 문화와 관련된 문제를 전문적으로 풀이해주는 문제 해설가입니다.  \n",
    "사용자가 입력한 문제에 대해 단계별로 차근차근(step by step) 설명하여 **문제 해설**과 **정답**을 제시하세요.  \n",
    "\n",
    "답변 형식은 반드시 다음과 같이 작성하세요:  \n",
    "문제 해설: (문제에 대한 배경, 개념 설명, 선택지 또는 정답 후보 분석 등 단계별로 논리적인 추론 과정을 포함)  \n",
    "정답: (정답만 간결하게 작성)\n",
    "\n",
    "※ 정답 작성 형식 안내  \n",
    "- 선다형 문제일 경우: **정답 번호**만 작성하세요.  \n",
    "- 단답형 문제일 경우:  \n",
    "    - 정답이 1개인 경우: 괄호, 한자 없이 **한글** 또는 **영어**를 사용하세요. (예: 사과 또는 apple)  \n",
    "    - 정답이 여러 개인 경우: 쉼표(,)로 구분해 나열하세요. (예: 사과, 배)  \n",
    "    - 순서 배열 문제인 경우: '-'로 구분해 정확한 순서를 유지해 나열하세요. (예: ㄱ-ㄴ-ㄷ-ㄹ)\"\"\"\n",
    "\n",
    "user_prompt=\"\"\"아래 문제를 단계별로 자세히 해설해주고, 마지막에 정답을 작성해줘.  \n",
    "\n",
    "키워드: {topic_keyword}  \n",
    "문제 유형: {question_type}  \n",
    "문제: {question}\"\"\"\n",
    "\n",
    "answer_tag=\"정답:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = user_prompt.format(\n",
    "    category=category,\n",
    "    domain=domain,\n",
    "    topic_keyword=topic_keyword,\n",
    "    question_type=question_type,\n",
    "    question=question\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7a03c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fadd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd93454",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f40c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
